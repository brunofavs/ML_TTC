
@article{fcn,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	volume = {39},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/7478072},
	doi = {10.1109/TPAMI.2016.2572683},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks ({AlexNet}, the {VGG} net, and {GoogLeNet}) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of {PASCAL} {VOC} (30\% relative improvement to 67.2\% mean {IU} on 2012), {NYUDv}2, {SIFT} Flow, and {PASCAL}-Context, while inference takes one tenth of a second for a typical image.},
	pages = {640--651},
	number = {4},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	urldate = {2025-02-26},
	date = {2017-04},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Image segmentation, Training, Computer architecture, Convolution, Convolutional Networks, Deep Learning, Fuses, Proposals, Semantic Segmentation, Semantics, Transfer Learning},
	file = {IEEE Xplore Abstract Record:/home/gribeiro/Zotero/storage/PHVF3BY5/7478072.html:text/html;Submitted Version:/home/gribeiro/Zotero/storage/8H8X8EYH/Shelhamer et al. - 2017 - Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf},
}

@misc{DeepLabV3,
	title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `{DeepLabv}3' system significantly improves over our previous {DeepLab} versions without {DenseCRF} post-processing and attains comparable performance with other state-of-art models on the {PASCAL} {VOC} 2012 semantic image segmentation benchmark.},
	number = {{arXiv}:1706.05587},
	publisher = {{arXiv}},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	urldate = {2025-02-26},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.05587 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/gribeiro/Zotero/storage/UKLUGL7E/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf:application/pdf;Snapshot:/home/gribeiro/Zotero/storage/HUIYRM2U/1706.html:text/html},
}

@inproceedings{LRASPP,
	title = {Searching for {MobileNetV}3},
	url = {https://ieeexplore.ieee.org/document/9008835},
	doi = {10.1109/ICCV.2019.00140},
	abstract = {We present the next generation of {MobileNets} based on a combination of complementary search techniques as well as a novel architecture design. {MobileNetV}3 is tuned to mobile phone {CPUs} through a combination of hardware-aware network architecture search ({NAS}) complemented by the {NetAdapt} algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new {MobileNet} models for release: {MobileNetV}3-Large and {MobileNetV}3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling ({LR}-{ASPP}). We achieve new state of the art results for mobile classification, detection and segmentation. {MobileNetV}3-Large is 3.2\% more accurate on {ImageNet} classification while reducing latency by 20\% compared to {MobileNetV}2. {MobileNetV}3-Small is 6.6\% more accurate compared to a {MobileNetV}2 model with comparable latency. {MobileNetV}3-Large detection is over 25\% faster at roughly the same accuracy as {MobileNetV}2 on {COCO} detection. {MobileNetV}3-Large {LR}-{ASPP} is 34\% faster than {MobileNetV}2 R-{ASPP} at similar accuracy for Cityscapes segmentation.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {1314--1324},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	author = {Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and Adam, Hartwig and Le, Quoc},
	urldate = {2025-02-26},
	date = {2019-10},
	note = {{ISSN}: 2380-7504},
	keywords = {Neural networks, Computational modeling, Image segmentation, Computer architecture, Proposals, Mobile handsets, Next generation networking},
	file = {IEEE Xplore Abstract Record:/home/gribeiro/Zotero/storage/EBPCKDPK/9008835.html:text/html},
}

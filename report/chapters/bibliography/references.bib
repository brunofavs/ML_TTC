
@article{fcn,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	volume = {39},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/7478072},
	doi = {10.1109/TPAMI.2016.2572683},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks ({AlexNet}, the {VGG} net, and {GoogLeNet}) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of {PASCAL} {VOC} (30\% relative improvement to 67.2\% mean {IU} on 2012), {NYUDv}2, {SIFT} Flow, and {PASCAL}-Context, while inference takes one tenth of a second for a typical image.},
	pages = {640--651},
	number = {4},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	urldate = {2025-02-26},
	date = {2017-04},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Image segmentation, Training, Computer architecture, Convolution, Convolutional Networks, Deep Learning, Fuses, Proposals, Semantic Segmentation, Semantics, Transfer Learning},
	file = {IEEE Xplore Abstract Record:/home/gribeiro/Zotero/storage/PHVF3BY5/7478072.html:text/html;Submitted Version:/home/gribeiro/Zotero/storage/8H8X8EYH/Shelhamer et al. - 2017 - Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf},
}

@misc{DeepLabV3,
	title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `{DeepLabv}3' system significantly improves over our previous {DeepLab} versions without {DenseCRF} post-processing and attains comparable performance with other state-of-art models on the {PASCAL} {VOC} 2012 semantic image segmentation benchmark.},
	number = {{arXiv}:1706.05587},
	publisher = {{arXiv}},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	urldate = {2025-02-26},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.05587 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/gribeiro/Zotero/storage/UKLUGL7E/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf:application/pdf;Snapshot:/home/gribeiro/Zotero/storage/HUIYRM2U/1706.html:text/html},
}

@inproceedings{LRASPP,
	title = {Searching for {MobileNetV}3},
	url = {https://ieeexplore.ieee.org/document/9008835},
	doi = {10.1109/ICCV.2019.00140},
	abstract = {We present the next generation of {MobileNets} based on a combination of complementary search techniques as well as a novel architecture design. {MobileNetV}3 is tuned to mobile phone {CPUs} through a combination of hardware-aware network architecture search ({NAS}) complemented by the {NetAdapt} algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new {MobileNet} models for release: {MobileNetV}3-Large and {MobileNetV}3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling ({LR}-{ASPP}). We achieve new state of the art results for mobile classification, detection and segmentation. {MobileNetV}3-Large is 3.2\% more accurate on {ImageNet} classification while reducing latency by 20\% compared to {MobileNetV}2. {MobileNetV}3-Small is 6.6\% more accurate compared to a {MobileNetV}2 model with comparable latency. {MobileNetV}3-Large detection is over 25\% faster at roughly the same accuracy as {MobileNetV}2 on {COCO} detection. {MobileNetV}3-Large {LR}-{ASPP} is 34\% faster than {MobileNetV}2 R-{ASPP} at similar accuracy for Cityscapes segmentation.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {1314--1324},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	author = {Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and Adam, Hartwig and Le, Quoc},
	urldate = {2025-02-26},
	date = {2019-10},
	note = {{ISSN}: 2380-7504},
	keywords = {Neural networks, Computational modeling, Image segmentation, Computer architecture, Proposals, Mobile handsets, Next generation networking},
	file = {IEEE Xplore Abstract Record:/home/gribeiro/Zotero/storage/EBPCKDPK/9008835.html:text/html},
}

@inproceedings{unet,
	location = {Cham},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	isbn = {978-3-319-24574-4},
	doi = {10.1007/978-3-319-24574-4_28},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	pages = {234--241},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	date = {2015},
	langid = {english},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	file = {Full Text PDF:/home/gribeiro/Zotero/storage/VY6BBFQD/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf},
}

@article{atom,
	title = {{ATOM}: A general calibration framework for multi-modal, multi-sensor systems},
	volume = {207},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012234},
	doi = {10.1016/j.eswa.2022.118000},
	shorttitle = {{ATOM}},
	abstract = {The fusion of data from different sensors often requires that an accurate geometric transformation between the sensors is known. The procedure by which these transformations are estimated is known as sensor calibration. The vast majority of calibration approaches focus on specific pairwise combinations of sensor modalities, unsuitable to calibrate robotic systems containing multiple sensors of varied modalities. This paper presents a novel calibration methodology which is applicable to multi-sensor, multi-modal robotic systems. The approach formulates the calibration as an extended optimization problem, in which the poses of the calibration patterns are also estimated. It makes use of a topological representation of the coordinate frames in the system, in order to recalculate the poses of the sensors throughout the optimization. Sensor poses are retrieved from the combination of geometric transformations which are atomic, in the sense that they are indivisible. As such, we refer to this approach as {ATOM} — Atomic Transformations Optimization Method. This makes the approach applicable to different calibration problems, such as sensor to sensor, sensor in motion, or sensor to coordinate frame. Additionally, the proposed approach provides advanced functionalities, integrated into {ROS}, designed to support the several stages of a complete calibration procedure. Results covering several robotic platforms and a large spectrum of calibration problems show that the methodology is in fact general, and achieves calibrations which are as accurate as the ones provided by state of the art methods designed to operate only for specific combinations of pairwise modalities.},
	pages = {118000},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Oliveira, Miguel and Pedrosa, Eurico and de Aguiar, André Pinto and Rato, Daniela Ferreira Pinto Dias and dos Santos, Filipe Neves and Dias, Paulo and Santos, Vitor},
	urldate = {2025-02-26},
	date = {2022-11-30},
	keywords = {Extrinsic calibration, Intrinsic calibration, Multi-modal, Multi-sensor, Registration, {ROS}},
	file = {ScienceDirect Snapshot:/home/gribeiro/Zotero/storage/AUAB36EF/S0957417422012234.html:text/html},
}

@article{camera_lidar,
	title = {A Camera to {LiDAR} calibration approach through the optimization of atomic transformations},
	volume = {176},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421003353},
	doi = {10.1016/j.eswa.2021.114894},
	abstract = {This paper proposes a camera-to-3D Light Detection And Ranging calibration framework through the optimization of atomic transformations. The system is able to simultaneously calibrate multiple cameras with Light Detection And Ranging sensors, solving the problem of Bundle. In comparison with the state-of-the-art, this work presents several novelties: the ability to simultaneously calibrate multiple cameras and {LiDARs}; the support for multiple sensor modalities; the calibration through the optimization of atomic transformations, without changing the topology of the input transformation tree; and the integration of the calibration framework within the Robot Operating System ({ROS}) framework. The software pipeline allows the user to interactively position the sensors for providing an initial estimate, to label and collect data, and visualize the calibration procedure. To test this framework, an agricultural robot with a stereo camera and a 3D Light Detection And Ranging sensor was used. Pairwise calibrations and a single calibration of the three sensors were tested and evaluated. Results show that the proposed approach produces accurate calibrations when compared to the state-of-the-art, and is robust to harsh conditions such as inaccurate initial guesses or small amount of data used in calibration. Experiments have shown that our optimization process can handle an angular error of approximately 20 degrees and a translation error of 0.5 meters, for each sensor. Moreover, the proposed approach is able to achieve state-of-the-art results even when calibrating the entire system simultaneously.},
	pages = {114894},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Pinto de Aguiar, André Silva and Riem de Oliveira, Miguel Armando and Pedrosa, Eurico Farinha and Neves dos Santos, Filipe Baptista},
	urldate = {2025-02-26},
	date = {2021-08-15},
	keywords = {Computer vision, Atomic transformations, Geometric optimization},
	file = {Full Text:/home/gribeiro/Zotero/storage/5BZNX2TM/Pinto de Aguiar et al. - 2021 - A Camera to LiDAR calibration approach through the optimization of atomic transformations.pdf:application/pdf;ScienceDirect Snapshot:/home/gribeiro/Zotero/storage/JUJFCQR7/S0957417421003353.html:text/html},
}
